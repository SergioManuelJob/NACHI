{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo previo en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = \"Dataset/train\"\n",
    "validation_directory = \"Dataset/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'88 AN': 0, 'ADMIRAL RED': 1, 'ADONIS': 2, 'AFRICAN GIANT SWALLOWTAIL': 3, 'AMERICAN SNOOT': 4, 'APPOLLO': 5, 'ATALA': 6, 'AWL BANDED COMMON': 7, 'BANDED GOLD': 8, 'BANDED HELICONIAN ORANGE': 9, 'BANDED PEACOCK': 10, 'BARRED FLASHER TWO': 11, 'BECKERS WHITE': 12, 'BIRDWING CAIRNS': 13, 'BLACK HAIRSTREAK': 14, 'BLUE CROW SPOTTED': 15, 'BLUE MORPHO': 16, 'BROWN SIPROETA': 17, 'CABBAGE WHITE': 18, 'CATTLEHEART CELLED GREEN': 19, 'CHECQUERED SKIPPER': 20, 'CHESTNUT': 21, 'CLEOPATRA': 22, 'CLOAK MOURNING': 23, 'CLODIUS PARNASSIAN': 24, 'CLOUDED SULPHUR': 25, 'COMA EASTERN': 26, 'COMMON WOOD-NYMPH': 27, 'COPPER PURPLISH': 28, 'COPPER TAIL': 29, 'CRACKER RED': 30, 'CRECENT': 31, 'CRIMSON PATCH': 32, 'DANAID EGGFLY': 33, 'DAPPLE EASTERN WHITE': 34, 'DOGFACE SOUTHERN': 35, 'EASTERN ELFIN PINE': 36, 'EGGFLY GREAT': 37, 'ELBOWED PIERROT': 38, 'GREAT JAY': 39, 'GREY HAIRSTREAK': 40, 'HAIRSTREAK PURPLE': 41, 'INDRA SWALLOW': 42, 'IPHICLUS SISTER': 43, 'JULIA': 44, 'KITE PAPER': 45, 'LADY PAINTED': 46, 'LARGE MARBLE': 47, 'LEAFWING TROPICAL': 48, 'LONG WING ZEBRA': 49, 'MALACHITE': 50, 'MANGROVE SKIPPER': 51, 'MARK QUESTION': 52, 'MESTRA': 53, 'METALMARK': 54, 'MILBERTS TORTOISESHELL': 55, 'MONARCH': 56, 'OAKLEAF ORANGE': 57, 'ORANGE SLEEPY': 58, 'ORANGE TIP': 59, 'ORCHARD SWALLOW': 60, 'PEACOCK': 61, 'PINE WHITE': 62, 'PIPEVINE SWALLOW': 63, 'POPINJAY': 64, 'POSTMAN RED': 65, 'PURPLE RED SPOTTED': 66, 'QUEEN STRAITED': 67, 'SATYR WOOD': 68, 'SCARCE SWALLOW': 69, 'SILVER SKIPPER SPOT': 70, 'SOOTYWING': 71, 'SWALLOW TAIL YELLOW': 72, 'ULYSES': 73, 'VICEROY': 74}\n",
      "Ejemplos de imágenes y etiquetas de entrenamiento:\n",
      "Imagen: [[1 0 2]\n",
      " [0 0 1]\n",
      " [2 0 4]]..., Etiqueta: 88 AN, Índice: 0\n",
      "\n",
      "Ejemplos de imágenes y etiquetas de validación:\n",
      "Imagen: [[ 0 39 26]\n",
      " [ 0 40 27]\n",
      " [ 0 40 27]]..., Etiqueta: 88 AN, Índice: 0\n"
     ]
    }
   ],
   "source": [
    "def cargar_imagenes_y_etiquetas(directorio):\n",
    "    imagenes = []\n",
    "    etiquetas = []\n",
    "    etiquetas_indices = {}\n",
    "    indice_actual = 0\n",
    "    \n",
    "    for root, _, files in os.walk(directorio):\n",
    "        for file in files:\n",
    "            # Verificar si el archivo es una imagen (puedes ajustar esta condición según el formato de tus imágenes)\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\") or file.endswith(\".jpeg\"):\n",
    "                # Obtener la ruta completa de la imagen\n",
    "                imagen_path = os.path.join(root, file)\n",
    "                # El nombre del directorio padre es la etiqueta (Y)\n",
    "                etiqueta = os.path.basename(root)\n",
    "                \n",
    "                # Si la etiqueta aún no está en el diccionario de índices, agregarla\n",
    "                if etiqueta not in etiquetas_indices:\n",
    "                    etiquetas_indices[etiqueta] = indice_actual\n",
    "                    indice_actual += 1\n",
    "                \n",
    "                # Cargar la imagen\n",
    "                imagen = cv2.imread(imagen_path)  \n",
    "                \n",
    "                # Agregar la imagen y su etiqueta a las listas correspondientes\n",
    "                imagenes.append(imagen)\n",
    "                etiquetas.append(etiqueta)\n",
    "    \n",
    "    # Devolver las imágenes, las etiquetas y el diccionario de índices de etiquetas\n",
    "    return imagenes, etiquetas, etiquetas_indices\n",
    "\n",
    "\n",
    "X_train, y_train, y_train_index = cargar_imagenes_y_etiquetas(train_directory)\n",
    "\n",
    "X_val, y_val, y_val_index = cargar_imagenes_y_etiquetas(validation_directory)\n",
    "\n",
    "print(\"Ejemplos de imágenes y etiquetas de entrenamiento:\")\n",
    "print(f\"Imagen: {X_train[1][1][:3]}..., Etiqueta: {y_train[1]}, Índice de la clase: {y_train_index.get(y_train[1])}\")\n",
    "\n",
    "print(\"\\nEjemplos de imágenes y etiquetas de validación:\")\n",
    "print(f\"Imagen: {X_val[1][1][:3]}..., Etiqueta: {y_val[1]}, Índice de la clase: {y_val_index.get(y_train[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_imagenes_por_clase(etiquetas):\n",
    "    conteo = Counter(etiquetas)\n",
    "    return conteo\n",
    "\n",
    "# Calcular el recuento de imágenes por clase en el conjunto de entrenamiento y validación\n",
    "count_train = contar_imagenes_por_clase(y_train)\n",
    "count_validation = contar_imagenes_por_clase(y_val)\n",
    "\n",
    "df_count_train = pd.DataFrame(list(count_train.items()), columns=['Clase', 'Total de imágenes'])\n",
    "df_count_validation = pd.DataFrame(list(count_validation.items()), columns=['Clase', 'Total de imágenes'])\n",
    "\n",
    "df_count_train.to_excel('Recursos/Recuentro Entrenamiento.xlsx', index=False)\n",
    "df_count_validation.to_excel('Recursos/Recuento Validación.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisando manualmente los datos en excel, me encuentro conque la clase menos popular es la crimson patch, con 96 imágenes. Y la clase más popular tiene 184 imágenes, esta corresponde a Cloak Mourning, tendremos en cuenta estos puntos para la revisión de las métricas del modelo, y ver cómo predice cada una.\n",
    "\n",
    "Dicho esto, procedemos a normalizar las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de la normalización: [[1 0 2]\n",
      " [0 0 1]\n",
      " [2 0 4]]\n",
      "Antes de la normalización: [[ 0 39 26]\n",
      " [ 0 40 27]\n",
      " [ 0 40 27]]\n",
      "Después de la normalización: [[0.00392157 0.         0.00784314]\n",
      " [0.         0.         0.00392157]\n",
      " [0.00784314 0.         0.01568627]]\n",
      "Después de la normalización: [[0.         0.15294118 0.10196078]\n",
      " [0.         0.15686275 0.10588235]\n",
      " [0.         0.15686275 0.10588235]]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "print(\"Antes de la normalización: \" + str(X_train[1][1][:3]))\n",
    "print(\"Antes de la normalización: \" + str(X_val[1][1][:3]))\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "print(\"Después de la normalización: \" + str(X_train[1][1][:3]))\n",
    "print(\"Después de la normalización: \" + str(X_val[1][1][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de training, X e Y respectivamente: 9092 , 9092\n",
      "Datos de validación, X e Y respectivamente: 1949 , 1949\n"
     ]
    }
   ],
   "source": [
    "print(\"Datos de training, X e Y respectivamente: \" + str(len(X_train)), \", \" + str(len(y_train)))\n",
    "print(\"Datos de validación, X e Y respectivamente: \" + str(len(X_val)), \", \" + str(len(y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignación de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=15\n",
    "img_rows,img_cols=180, 180 # Todas las imágenes vienen redimensionadas a 180x180 píxeles\n",
    "input_shape=(img_rows,img_cols,3)\n",
    "patience_factor = 2     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  Conv2D(32, kernel_size=(3, 3), data_format=\"channels_last\",input_shape=input_shape, activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  MaxPooling2D(pool_size=(2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dropout(0.25),\n",
    "  Dense(6, activation='softmax')\n",
    "])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience_factor, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '88 AN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m y_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_val)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convertir las etiquetas a un formato adecuado si es necesario (por ejemplo, one-hot encoding para clasificación multiclase)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m y_val \u001b[38;5;241m=\u001b[39m to_categorical(y_val)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\numerical_utils.py:77\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(x, num_classes)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mis_tensor(x):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mone_hot(x, num_classes)\n\u001b[1;32m---> 77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Shrink the last dimension if the shape is (..., 1).\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '88 AN'"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Convertir a one-hot encoding para esta clasificación multiclase\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[[[0.13333333 0.18823529 0.2627451 ]\n   [0.14117647 0.19607843 0.27058824]\n   [0.14901961 0.20392157 0.27843137]\n   ...\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]]\n\n  [[0.12941176 0.18431373 0.25882353]\n   [0.1372549  0.19215686 0.26666667]\n   [0.14509804 0.2        0.2745098 ]\n   ...\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]]\n\n  [[0.12941176 0.18431373 0.25882353]\n   [0.1372549  0.19215686 0.26666667]\n   [0.14509804 0.2        0.2745098 ]\n   ...\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]]\n\n  ...\n\n  [[0.16078431 0.29803922 0.10196078]\n   [0.16470588 0.30588235 0.10196078]\n   [0.16862745 0.30980392 0.09803922]\n   ...\n   [0.59215686 0.66666667 0.30196078]\n   [0.58823529 0.6627451  0.29803922]\n   [0.58823529 0.6627451  0.29803922]]\n\n  [[0.18431373 0.32941176 0.10980392]\n   [0.18823529 0.3372549  0.10980392]\n   [0.19215686 0.34117647 0.10588235]\n   ...\n   [0.57647059 0.65098039 0.28627451]\n   [0.57254902 0.64705882 0.28235294]\n   [0.57254902 0.64705882 0.28235294]]\n\n  [[0.18039216 0.32941176 0.09411765]\n   [0.18431373 0.3372549  0.09411765]\n   [0.18823529 0.34117647 0.09803922]\n   ...\n   [0.56862745 0.64313725 0.27843137]\n   [0.56470588 0.63921569 0.2745098 ]\n   [0.56078431 0.63529412 0.27058824]]]\n\n\n [[[0.00784314 0.00392157 0.01176471]\n   [0.         0.         0.00392157]\n   [0.00784314 0.         0.01568627]\n   ...\n   [0.16470588 0.31372549 0.14901961]\n   [0.16862745 0.29019608 0.1372549 ]\n   [0.         0.09019608 0.        ]]\n\n  [[0.00392157 0.         0.00784314]\n   [0.         0.         0.00392157]\n   [0.00784314 0.         0.01568627]\n   ...\n   [0.16862745 0.31372549 0.15686275]\n   [0.16862745 0.29019608 0.14509804]\n   [0.         0.08235294 0.        ]]\n\n  [[0.         0.         0.00392157]\n   [0.         0.         0.00392157]\n   [0.01176471 0.         0.01960784]\n   ...\n   [0.16470588 0.30588235 0.16470588]\n   [0.16470588 0.27843137 0.14901961]\n   [0.         0.07058824 0.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]]\n\n\n [[[0.53333333 0.66666667 0.62352941]\n   [0.53333333 0.65882353 0.61568627]\n   [0.55294118 0.65098039 0.61176471]\n   ...\n   [0.13333333 0.45098039 0.41568627]\n   [0.12941176 0.43921569 0.40392157]\n   [0.12156863 0.43137255 0.39607843]]\n\n  [[0.53333333 0.65882353 0.61568627]\n   [0.5372549  0.65490196 0.61176471]\n   [0.54901961 0.64705882 0.60784314]\n   ...\n   [0.14901961 0.44313725 0.41176471]\n   [0.14117647 0.43529412 0.40392157]\n   [0.1372549  0.43137255 0.4       ]]\n\n  [[0.54117647 0.65490196 0.61960784]\n   [0.53333333 0.64705882 0.61176471]\n   [0.54901961 0.64313725 0.61176471]\n   ...\n   [0.17254902 0.42352941 0.40392157]\n   [0.16862745 0.41960784 0.4       ]\n   [0.16470588 0.41568627 0.39607843]]\n\n  ...\n\n  [[0.15294118 0.59607843 0.33333333]\n   [0.15294118 0.59607843 0.33333333]\n   [0.16078431 0.59215686 0.33333333]\n   ...\n   [0.09411765 0.6        0.37647059]\n   [0.08627451 0.59215686 0.36862745]\n   [0.08627451 0.59215686 0.36862745]]\n\n  [[0.15294118 0.59607843 0.33333333]\n   [0.15294118 0.59607843 0.33333333]\n   [0.15686275 0.58823529 0.32941176]\n   ...\n   [0.11372549 0.61960784 0.39607843]\n   [0.10588235 0.61176471 0.38823529]\n   [0.10196078 0.60784314 0.38431373]]\n\n  [[0.15686275 0.6        0.3372549 ]\n   [0.15294118 0.59607843 0.33333333]\n   [0.15686275 0.58823529 0.32941176]\n   ...\n   [0.12941176 0.63529412 0.41176471]\n   [0.11764706 0.62352941 0.4       ]\n   [0.10980392 0.61568627 0.39215686]]]\n\n\n ...\n\n\n [[[0.         0.         0.02745098]\n   [0.01176471 0.02352941 0.04313725]\n   [0.         0.         0.00392157]\n   ...\n   [0.25490196 0.50196078 0.53333333]\n   [0.2        0.43529412 0.45882353]\n   [0.05882353 0.29411765 0.31764706]]\n\n  [[0.         0.         0.02745098]\n   [0.00784314 0.01960784 0.03921569]\n   [0.         0.         0.00392157]\n   ...\n   [0.25882353 0.50196078 0.5254902 ]\n   [0.18431373 0.41960784 0.44313725]\n   [0.03529412 0.27058824 0.29411765]]\n\n  [[0.         0.         0.02745098]\n   [0.         0.01176471 0.03137255]\n   [0.         0.         0.00392157]\n   ...\n   [0.23137255 0.4745098  0.49803922]\n   [0.14117647 0.37647059 0.4       ]\n   [0.01176471 0.23921569 0.25882353]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]]\n\n\n [[[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  ...\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]]\n\n\n [[[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [0.81176471 0.81176471 0.81176471]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [1.         1.         1.        ]\n   [0.99215686 0.99215686 0.99215686]\n   [0.80392157 0.80392157 0.80392157]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [1.         1.         1.        ]\n   [0.99215686 0.99215686 0.99215686]\n   [0.79607843 0.79607843 0.79607843]]]] (of type <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:120\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=[[[[0.13333333 0.18823529 0.2627451 ]\n   [0.14117647 0.19607843 0.27058824]\n   [0.14901961 0.20392157 0.27843137]\n   ...\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]]\n\n  [[0.12941176 0.18431373 0.25882353]\n   [0.1372549  0.19215686 0.26666667]\n   [0.14509804 0.2        0.2745098 ]\n   ...\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]]\n\n  [[0.12941176 0.18431373 0.25882353]\n   [0.1372549  0.19215686 0.26666667]\n   [0.14509804 0.2        0.2745098 ]\n   ...\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]\n   [0.14117647 0.17647059 0.23137255]]\n\n  ...\n\n  [[0.16078431 0.29803922 0.10196078]\n   [0.16470588 0.30588235 0.10196078]\n   [0.16862745 0.30980392 0.09803922]\n   ...\n   [0.59215686 0.66666667 0.30196078]\n   [0.58823529 0.6627451  0.29803922]\n   [0.58823529 0.6627451  0.29803922]]\n\n  [[0.18431373 0.32941176 0.10980392]\n   [0.18823529 0.3372549  0.10980392]\n   [0.19215686 0.34117647 0.10588235]\n   ...\n   [0.57647059 0.65098039 0.28627451]\n   [0.57254902 0.64705882 0.28235294]\n   [0.57254902 0.64705882 0.28235294]]\n\n  [[0.18039216 0.32941176 0.09411765]\n   [0.18431373 0.3372549  0.09411765]\n   [0.18823529 0.34117647 0.09803922]\n   ...\n   [0.56862745 0.64313725 0.27843137]\n   [0.56470588 0.63921569 0.2745098 ]\n   [0.56078431 0.63529412 0.27058824]]]\n\n\n [[[0.00784314 0.00392157 0.01176471]\n   [0.         0.         0.00392157]\n   [0.00784314 0.         0.01568627]\n   ...\n   [0.16470588 0.31372549 0.14901961]\n   [0.16862745 0.29019608 0.1372549 ]\n   [0.         0.09019608 0.        ]]\n\n  [[0.00392157 0.         0.00784314]\n   [0.         0.         0.00392157]\n   [0.00784314 0.         0.01568627]\n   ...\n   [0.16862745 0.31372549 0.15686275]\n   [0.16862745 0.29019608 0.14509804]\n   [0.         0.08235294 0.        ]]\n\n  [[0.         0.         0.00392157]\n   [0.         0.         0.00392157]\n   [0.01176471 0.         0.01960784]\n   ...\n   [0.16470588 0.30588235 0.16470588]\n   [0.16470588 0.27843137 0.14901961]\n   [0.         0.07058824 0.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]]\n\n\n [[[0.53333333 0.66666667 0.62352941]\n   [0.53333333 0.65882353 0.61568627]\n   [0.55294118 0.65098039 0.61176471]\n   ...\n   [0.13333333 0.45098039 0.41568627]\n   [0.12941176 0.43921569 0.40392157]\n   [0.12156863 0.43137255 0.39607843]]\n\n  [[0.53333333 0.65882353 0.61568627]\n   [0.5372549  0.65490196 0.61176471]\n   [0.54901961 0.64705882 0.60784314]\n   ...\n   [0.14901961 0.44313725 0.41176471]\n   [0.14117647 0.43529412 0.40392157]\n   [0.1372549  0.43137255 0.4       ]]\n\n  [[0.54117647 0.65490196 0.61960784]\n   [0.53333333 0.64705882 0.61176471]\n   [0.54901961 0.64313725 0.61176471]\n   ...\n   [0.17254902 0.42352941 0.40392157]\n   [0.16862745 0.41960784 0.4       ]\n   [0.16470588 0.41568627 0.39607843]]\n\n  ...\n\n  [[0.15294118 0.59607843 0.33333333]\n   [0.15294118 0.59607843 0.33333333]\n   [0.16078431 0.59215686 0.33333333]\n   ...\n   [0.09411765 0.6        0.37647059]\n   [0.08627451 0.59215686 0.36862745]\n   [0.08627451 0.59215686 0.36862745]]\n\n  [[0.15294118 0.59607843 0.33333333]\n   [0.15294118 0.59607843 0.33333333]\n   [0.15686275 0.58823529 0.32941176]\n   ...\n   [0.11372549 0.61960784 0.39607843]\n   [0.10588235 0.61176471 0.38823529]\n   [0.10196078 0.60784314 0.38431373]]\n\n  [[0.15686275 0.6        0.3372549 ]\n   [0.15294118 0.59607843 0.33333333]\n   [0.15686275 0.58823529 0.32941176]\n   ...\n   [0.12941176 0.63529412 0.41176471]\n   [0.11764706 0.62352941 0.4       ]\n   [0.10980392 0.61568627 0.39215686]]]\n\n\n ...\n\n\n [[[0.         0.         0.02745098]\n   [0.01176471 0.02352941 0.04313725]\n   [0.         0.         0.00392157]\n   ...\n   [0.25490196 0.50196078 0.53333333]\n   [0.2        0.43529412 0.45882353]\n   [0.05882353 0.29411765 0.31764706]]\n\n  [[0.         0.         0.02745098]\n   [0.00784314 0.01960784 0.03921569]\n   [0.         0.         0.00392157]\n   ...\n   [0.25882353 0.50196078 0.5254902 ]\n   [0.18431373 0.41960784 0.44313725]\n   [0.03529412 0.27058824 0.29411765]]\n\n  [[0.         0.         0.02745098]\n   [0.         0.01176471 0.03137255]\n   [0.         0.         0.00392157]\n   ...\n   [0.23137255 0.4745098  0.49803922]\n   [0.14117647 0.37647059 0.4       ]\n   [0.01176471 0.23921569 0.25882353]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]]\n\n\n [[[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  ...\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]\n\n  [[1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]]]\n\n\n [[[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]]\n\n  ...\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [1.         1.         1.        ]\n   [1.         1.         1.        ]\n   [0.81176471 0.81176471 0.81176471]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [1.         1.         1.        ]\n   [0.99215686 0.99215686 0.99215686]\n   [0.80392157 0.80392157 0.80392157]]\n\n  [[0.         0.         0.        ]\n   [0.         0.         0.        ]\n   [0.         0.         0.        ]\n   ...\n   [1.         1.         1.        ]\n   [0.99215686 0.99215686 0.99215686]\n   [0.79607843 0.79607843 0.79607843]]]] (of type <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
